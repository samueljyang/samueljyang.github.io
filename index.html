<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=900,
    initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Samuel J. Yang</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/scrolling-nav.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
<style>
body
{
font-size:19px;
}
</style>

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll"
    href="#page-top">Samuel J. Yang</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a class="page-scroll" href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#news">News</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publications">Publications</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Section -->
    <section id="intro" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
		           <img class="img-responsive"
    src="img/sam2_circle-1.png" alt="" style="float:left">
		                      <h1>Samuel J. Yang</h1>
		  <p>Samuel J. Yang joined <a href="https://ai.google.com/">Google Research</a> in
    2016. Prior to that, he completed a Ph.D. in Electrical
    Engineering at Stanford University, where his research in the labs of
    <a href="http://web.stanford.edu/group/dlab/research.html">Karl Deisseroth</a> and     <a href="http://www.computationalimaging.org/research-overview/">Gordon
    Wetzstein</a> focused on computational imaging and display, the
    co-design and optimization of optics hardware and data processing alogrithms. He was supported by a <a
    href="https://www.nsfgrfp.org/">NSF Graduate Research
    Fellowship</a> and a <a href="https://ndseg.asee.org/">NDSEG
    Graduate Fellowship</a>.</p>
                    <a class="btn btn-default page-scroll" href="#about">About</a>
                </div>
            </div>
        </div>
    </section>

    <!-- About Section -->
    <section id="about" class="about-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>About</h1>
		  <p>Before completing my Ph.D., I received a B.S. in Electrical Engineering
    from <a href="http://www.caltech.edu/">Caltech</a>, studying
    engineering physics and optics in
    <a
    href="http://www.biophot.caltech.edu/people/index.html">Changhuei
    Yang</a>'s lab, and a M.S. in
    Electrical Engineering from <a href="https://ee.stanford.edu/">Stanford</a>, studying machine learning,
    image processing and computer vision. </p>
		  		  <p>In 2015, at  <a
    href="https://research.google.com/">Google Research</a>, I applied
    deep learning methods to images as a Software Engineering Intern.</p>
		  		  <p>In 2014, at  <a
    href="http://www.google.com/about/company/">Google [x]</a>, I
     worked with optical physicists to design and implement
    imaging instrumentation hardware as an intern.</p>
		  <p>In 2013, at  <a
    href="http://www.pelicanimaging.com/">Pelican Imaging</a>, I
    explored 
    <a
    href="http://image-sensors-world.blogspot.com/2015/04/pelican-imaging-proposes-depth-assisted.html">computational
    photography applications</a> as a research intern.</p>

<p><strong>Contact:</strong> samuely (at) alumni (dot) stanford (dot) edu</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Services Section -->
    <section id="news" class="services-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">

		  <h1>News</h1>
		  <p><strong>June 2023</strong>: Added Venugopalan et
		  al. 2023.</p>
		  <p><strong>September 2022</strong>: Added <a
    href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/yang22u_interspeech.pdf">Yang et
		  al. 2022 at Interspeech 2022</a>.</p>
		   <p><strong>March 2022</strong>: Added Schiff et
		  al. 2022.</p>
		  <p><strong>January 2022</strong>: Added Cooke et al. 2021.</p>
		  <p><strong>September 2021</strong>: Added Yang et
		  al. 2021 and associated <a
		  href="https://www.caltech.edu/about/news/finding-a-metal-oxide-needle-in-a-periodic-table-haystack">press release</a>. 
		  		  <p><strong>July 2021</strong>: See
		  		  blog post on <a href="https://ai.googleblog.com/2021/07/applying-advanced-speech-enhancement-in.html">applying speech
		  		  enhancement for cochlear implants</a>.</p>
		  <p><strong>July 2021</strong>: Added Wright & Yang 2021.</p>
		  <p><strong>2020</strong>: Added Schiff et al., 2020
		  and Tabak et al., 2020.</p>
		  <p><strong>2019</strong>: Added Yang et al., 2019 and Andalman et al., 2019.</p>
		  <p><strong>2018</strong>: <a
		  href="https://ai.googleblog.com/2018/04/seeing-more-with-in-silico-labeling-of.html"><i>In Silico</i> Labeling</a> is out in <i>Cell</i>. Updated with <a
		  href="https://ai.googleblog.com/2018/03/using-deep-learning-to-facilitate.html">blog
		  post</a> and 4 recent publications.</p>
		  <p><strong>2016</strong>: I presented <a
		  href="http://www.focusonmicroscopy.org/2016/PDF/16068_Yang.pdf">this work</a>
		  at <a
		  href="http://www.focusonmicroscopy.org/">Focus on
		  Microscopy 2016</a>. Added two
		  computer vision/machine learning projects, <a
		  href="http://samueljyang.xyz/pdfs/Yang_2012_unpublished.pdf
		  ">real-time tail/eye tracking for zebrafish virtual reality</a> and <a
		  href="http://image-sensors-world.blogspot.com/2015/04/pelican-imaging-proposes-depth-assisted.html">depth-assisted
		  portrait perspective correction</a>.		  		 Our multifiber recording paper is out
		  		  in <a
		  		  href="http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.3770.html">Nature
		  		  Methods</a>, with software released
		  		  on <a href="https://github.com/deisseroth-lab/multifiber/">GitHub</a>. </p>
		  <p><strong>2015</strong>: Our light sheet
		  microscopy paper is out 
		  at <a
		  href="http://www.cell.com/cell/abstract/S0092-8674(15)01623-2">Cell</a>. My paper is out
		  at <a
		  href="https://www.osapublishing.org/oe/abstract.cfm?uri=oe-23-25-32573">Optics express</a>. I presented <a
    href="http://www.abstractsonline.com/Plan/ViewAbstract.aspx?sKey=38912891-fd0c-4e7c-a0fb-ba252be79840&cKey=35811712-6d48-45a2-bbd5-8f0beab36e65&mKey=%7bD0FF4555-8574-4FBB-B9D4-04EEC8BA0C84%7d">this
    poster</a> at SFN 2015. I also contributed to work in <a
		    href="http://www.abstractsonline.com/plan/ViewAbstract.aspx?mID=3744&sKey=3e89f7de-5fa8-4e46-9521-5857e110a864&cKey=a9c83cc4-2fc7-4a28-b3fd-628d012cdc23&mKey=d0ff4555-8574-4fbb-b9d4-04eec8ba0c84">this
		    poster</a>. Our <a
    href="http://www.computationalimaging.org/publications/adaptive-spectral-projection-siggraph-asia-2015/">adaptive
    spectral projector</a> was presented at <a href="http://sa2015.siggraph.org/">SIGGRAPH Asia 2015</a>.
                </div>
            </div>
        </div>
    </section>

    <!-- Contact Section -->
    <section id="publications" class="contact-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>Publications</h1>
		  <ol align="left">
<p>I am also on <a
href="https://scholar.google.com/citations?user=93Pw8u8AAAAJ">Google
Scholar</a>, <a
href="https://www.researchgate.net/profile/Samuel_J_Yang">ResearchGate</a>
		    and <a
		    href="https://github.com/samueljyang">GitHub</a>.</p>
		    <li>Venugopalan, S., Tobin, J., <u>Yang,
    S. J.</u>, Seaver, K., Cave, J.N.R., Jiang, P.P., Zeghidour, N.,
    Heywood, R., Green, J., Brenner, M.P. (2023). Speech Intelligibility Classifiers From 550K Disordered Speech Samples. <i>ICASSP 2023</i>. [ <a
    href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095933">PDF</a>
    		  ]</li>
		    <li><u>Yang,
    S. J.</u>, Wisdom, S., Gnegy, Chet., Lyon, R.F., Savla, Sagar. (2022). Listening with Googlears: Low-Latency Neural Multiframe Beamforming and
Equalization for Hearing Aids. <i>Interspeech 2022</i>. [ <a
    href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/yang22u_interspeech.pdf">PDF</a>
    		   | <a
    href="https://claritychallenge.github.io/clarity2021-workshop/programme.html">Clarity
    workshop 2021 version
    link</a> ]</li>
		    <li>Schiff, L.*, Migliori, B.*, Chen, Y.*,
    Carter, D.*, Bonilla, C., Hall, J., Fan, M.,
    Tam, E., Ahadi, S., Fischbacher, B., Geraschenko, A.,
    Hunter, C. J., Venugopalan, S., DesMarteau, S.,
    Narayanaswamy, A., Jacob, S., Armstrong, Z.,
    Ferrarotto, P., Williams, B., Buckley-Herd, G., Hazard, J.,
    Goldberg, J., Coram, M., Otto, R., Baltz, E. A., 
    Andres-Martin, L., Pritchard, O., Duren-Lubanski, A., Daigavane, A.
    Reggio, K., NYSCF Global Stem Cell Array Team, Nelson, P.C.,
    Frumkin, M., Solomon, S.L., Bauer, L., 
    Aiyar, R. S., Schwarzbach, E., Noggle, S. A.,
    Monsma, Jr., F. J., Paull, D., Berndl, M.**, <u>Yang, S. J.**</u>,
    Johannesson, B.** (2022). Integrating deep learning and unbiased
    automated high-content screening to identify complex disease
    signatures in human fibroblasts. <i>Nature Communications</i>. [ <a
		    href="https://nyscf.org/resources/artificial-intelligence-and-robotics-uncover-hidden-signatures-of-parkinsons-disease/">press
    release</a>
		    | <a
		    href="https://rdcu.be/cJNCM">PDF</a>
		    | <a
    href="https://rdcu.be/cJNCM">link</a>
    | <a
    href="https://www.biorxiv.org/content/10.1101/2020.11.13.380576v3">biorxiv</a>
    		    ]</li>
		    <li>Cooke, C.L., Kong, F., Chaware, A., Zhou,
    K.C., Kim, K., Xu, R., Ando, D.M., <u>Yang,
    S. J.</u>, Konda, P.C., Horstmeyer, R. (2021). Physics-Enhanced Machine Learning for Virtual Fluorescence Microscopy. <i>International Conference on
Computer Vision ICCV2021</i>. [ <a
    href="https://openaccess.thecvf.com/content/ICCV2021/papers/Cooke_Physics-Enhanced_Machine_Learning_for_Virtual_Fluorescence_Microscopy_ICCV_2021_paper.pdf">PDF</a>
    		   | <a
    href="https://openaccess.thecvf.com/content/ICCV2021/html/Cooke_Physics-Enhanced_Machine_Learning_for_Virtual_Fluorescence_Microscopy_ICCV_2021_paper.html">link</a> ]</li>
		    <li>Yang, L., Haber, J. A., Armstrong, Z., <u>Yang,
    S. J.</u>, Kan, K., Zhou, L., Richter, M. H., Roat, C., Wagner,
    N., Coram, M., Berndl, M., Riley, P., Gregoire,
    J. M. (2021). Discovery of complex oxides via automated
    experiments and data science. <i>PNAS</i>. [ <a
    href="https://www.pnas.org/content/pnas/118/37/e2106042118.full.pdf">PDF</a>
    		   | <a
    href="https://doi.org/10.1073/pnas.2106042118">link</a> | <a
    href="https://www.caltech.edu/about/news/finding-a-metal-oxide-needle-in-a-periodic-table-haystack">press
    release</a> ]</li>
		    <li>Wright, Carrie.  <u>Yang,
    S. J.</u> (2021). Deep learning for automated focus quality
    detection in wafer inspection. <i>SPIE Optical Metrology</i>. [ <a
    href="http://dx.doi.org/10.1117/12.2592425">link</a>
    		    ]</li>
		    
		    <li>Tabak, G., Fan, M., <u>Yang, S. J.</u>, Hoyer,
    S., & Davis, G.. (2020). Correcting nuisance variation using Wasserstein distance. <i>PeerJ</i>. [ <a
    href="https://peerj.com/articles/8594.pdf">PDF</a>
    | <a
    href="https://peerj.com/articles/8594/">link</a> | <a
    href="https://arxiv.org/abs/1711.00882">old arXiv version</a>
		    ]</li>
		    <li>Venugopalan, S.*, Narayanaswamy, A.*, <u>Yang, S.*</u>, Geraschenko, A., Lipnick,
    S., Makhortova, N. R., Hawrot, J., Marques, C., Pereira, J.,
    Brenner, M., Rubin, L., Wainger, B., Berndl, M. (2019). It's easy to fool
    yourself: Case studies on identifying bias and confounding in
    bio-medical datasets. <i>Neurips 2019 LMRL workshop, extended abstract</i>. [ <a
    href="https://arxiv.org/pdf/1912.07661.pdf">PDF</a>
    | <a
    href="https://arxiv.org/abs/1912.07661">link</a>
    ] </li>
		    <li><u>Yang, S. J.*</u>, Lipnick,
    S. L.*, Makhortova, N. R.*, Venugopalan, S.*, Fan, M.*, Armstrong, Z.,
    Schlaeger, T. M., Deng, L., Chung, W. K., O'Callaghan, L.,
    Geraschenko, A., Whye, D., Berndl, M., Hazard, J., Williams, B.,
    Narayanaswamy, A., Ando, D. M., Nelson, P. & Rubin, L. L. (2019). Applying Deep Neural Network
    Analysis to High-Content Image-Based Assays. <i>SLAS Discovery</i>. [ <a
    href="https://journals.sagepub.com/doi/pdf/10.1177/2472555219857715">PDF</a>
    | <a
    href="https://doi.org/10.1177/2472555219857715">link</a>
    ] </li>
		    <li>Andalman, A. S., Burns, V. M., Lovett-Barron,
    M., Broxton, M., Poole, B., <u>Yang, S. J.</u>, Grosenick, L.,
    Lerner, T. N., Chen, R., Benster, T., Mourrain, P., Levoy, M.,
    Rajan, K. & Deisseroth, K. (2019). Neuronal dynamics regulating
    brain and behavioral state transitions. <i>Cell</i>. [ <a
    href="https://web.stanford.edu/group/dlab/media/papers/andalmanCell2019.pdf">PDF</a>
    | <a
    href="https://www.sciencedirect.com/science/article/pii/S009286741930220X">link</a>
    ] </li>
		    <li>Christiansen, E. M., <u>Yang, S. J.</u>, Ando, D. M., Javaherian, A.,
		      Skibinski, G., Lipnick, S., Mount, S., O'Neil, A., Shah, K., Lee, A. K., Goyal, P.,
Fedus, W., Poplin, R., Esteva, A., Berndl, M., Rubin, L. L., Nelson, P., & Finkbeiner, S.
 (2018). <i>In Silico</i> Labeling: Predicting Fluorescent Labels in Unlabeled Images.
   <i>Cell</i>. [ <a
    href="https://www.cell.com/cell/pdf/S0092-8674(18)30364-7.pdf">PDF</a>
    | <a
    href="https://doi.org/10.1016/j.cell.2018.03.040">link</a>
    | <a href="https://ai.googleblog.com/2018/04/seeing-more-with-in-silico-labeling-of.html">blog post</a>
    | <a href="https://www.wired.com/story/ai-learns-a-new-trick-measuring-brain-cells/">in Wired</a>
    | <a href="https://www.nih.gov/news-events/news-releases/scientists-teach-computers-how-analyze-brain-cells">from NIH</a>
    | <a href="https://gladstone.org/about-us/news/deep-learning-superhuman-way-look-cells/">from Gladstone</a>
		    ]</li>
		    <li><u>Yang, S. J.</u>, Berndl, M., Ando, D. M.,
    Barch, M., Narayanaswamy, A., Christiansen, E., Hoyer, S., Roat,
    C., Hung, J., Rueden, C. T., Shankar, A., Finkbeiner, S., &
    Nelson, P. (2018). Assessing microscope image focus quality with
    deep learning. <i>BMC Bioinformatics</i>, 19(1). [ <a
    href="http://rdcu.be/I5cE">PDF</a>
    | <a
    href="https://doi.org/10.1186/s12859-018-2087-4">link</a>
    | <a href="https://ai.googleblog.com/2018/03/using-deep-learning-to-facilitate.html">blog post</a>
		    ]</li>
		    <li>Allen, W.E., Kauvar, I.V., Chen, M.Z.,
    Richman, E.B., <u>Yang, S. J.</u>, Chan, K., Gradinaru, V.,
    Deverman, B.E., Luo, L., & Deisseroth, K. (2017). Global
    Representations of Goal-Directed Behavior in Distinct Cell Types
    of Mouse Neocortex. <i>Neuron</i>, 94(4). [ <a
    href="https://web.stanford.edu/group/dlab/media/papers/allenNeuron2017.pdf">PDF</a>
    | <a
    href="https://www.sciencedirect.com/science/article/pii/S0896627317303434">link</a>
		    ]</li>
		    <li>Grosenick, L.M., Broxton, M., Kim, C.K.,
    Liston, C., Poole, B., <u>Yang, S.</u>, Andalman, A.S., Scharff,
    E., Cohen, N., Yizhar, O., Ramakrishnan, C., Ganguli, S., Suppes,
    P., Levoy, M., & Deisseroth, K. (2017). Identification Of Cellular-Activity Dynamics Across Large Tissue Volumes In The Mammalian Brain. <i>bioRxiv</i>, 94(4). [ <a
    href="https://www.biorxiv.org/content/biorxiv/early/2017/05/01/132688.full.pdf">PDF</a>
    | <a
    href="https://doi.org/10.1101/132688">link</a>
    ]</li>
		    <li>Kim, C.*, <u>Yang, S.*</u>, Pichamoorthy, N.,
    Young, N., Kauvar, I., Jennings, J., Lerner, T., Berndt, A., Lee,
    S.Y., Ramakrishnan, C., Davidson, T., Inoue, M., Bito, H., &
    Deisseroth, K. (2016). Simultaneous fast measurement of circuit
    dynamics at multiple sites across the mammalian brain. <i>Nature
    Methods</i>, 13(4). *co-first authors [ <a
    href="http://web.stanford.edu/group/dlab/media/papers/kimNMeth2016.pdf">PDF</a>
    | <a
    href="http://web.stanford.edu/group/dlab/media/papers/kimNMeth2016supp.pdf">supplement</a>
    | <a
    href="http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.3770.html">link</a>
    | <a href="https://github.com/deisseroth-lab/multifiber/">software</a>
    ]</li>
		    <li>Tomer, R., Lovett-Barron, M., Kauvar, I.,
    Andalman, A., Burns, V.M., Sankaran, S., Grosenick, L., Broxton,
    M., <u>Yang, S.</u> & Deisseroth, K. (2015). SPED Light Sheet Microscopy: Fast Mapping of Biological System Structure and Function. <i>Cell</i>, 163(7), 0092-8674.  [ <a
    href="http://web.stanford.edu/group/dlab/media/papers/tomerCellResource2015.pdf">PDF</a>
    | <a href="http://www.cell.com/cell/abstract/S0092-8674(15)01623-2">link</a>
    ]</li>
    
    <li><u>Yang, S.</u>, Allen, W., Kauvar, I.,
    Andalman, A., Young, N., Kim, C., Marshel, J., Wetzstein,
    G., & Deisseroth, K. (2015). Extended field-of-view and
    increased-signal 3D holographic illumination with time-division
    multiplexing. <i>Optics express</i>, 23(25), 32573-32581.  [ <a
    href="http://web.stanford.edu/group/dlab/media/papers/yangOpticsExpress2015.pdf">PDF</a>
    | <a href="https://www.osapublishing.org/vjbo/fulltext.cfm?uri=oe-23-25-32573">link</a>
    ]</li>
    
<li>Kauvar, I., <u>Yang, S.</u>, Shi, L., McDowall, I., &
    Wetzstein, G. (2015). Adaptive Color Display via Perceptually-driven
    Factored Spectral Projection. <i>ACM SIGGRAPH Asia (Transactions
    on Graphics)</i>. [ <a
    href="http://www.computationalimaging.org/wp-content/uploads/2015/08/AdaptiveSpectralProjection-SIGAsia2015.pdf">PDF</a>
    | <a href="http://www.computationalimaging.org/publications/adaptive-spectral-projection-siggraph-asia-2015/">link</a>
    ]</li>
        		    
    <li>Cohen, N., <u>Yang, S.</u>, Andalman, A., Broxton, M., Grosenick, L.,
    Deisseroth, K., Horowitz, M., & Levoy, M. (2014). Enhancing the performance
    of the light field microscope using wavefront coding. <i>Optics
    express</i>, 22(20), 24817-24839. [ <a
    href="http://graphics.stanford.edu/papers/lfmwavefrontcoding/cohen_oe-22-20-24817.pdf">PDF</a>
    | <a href="https://www.osapublishing.org/vjbo/fulltext.cfm?uri=oe-22-20-24817">link</a>
    ]</li>
       

    <li>Broxton, M., Grosenick, L., <u>Yang, S.</u>, Cohen, N., Andalman, A.,
    Deisseroth, K., & Levoy, M. (2013). Wave optics theory and 3-D
    deconvolution for the light field microscope. <i>Optics express</i>,
    21(21), 25418-25439. [ <a
    href="http://graphics.stanford.edu/papers/lfdeconvolution/broxton_oe-21-21-25418.pdf">PDF</a>
    | <a href="https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-21-21-25418">link</a>
    ]</li>
      

    <li>Lee, S. A., Leitao, R., Zheng, G., <u>Yang, S.</u>, Rodriguez, A., &
    Yang, C. (2011). Color capable sub-pixel resolving optofluidic
    microscope and its application to blood cell imaging for malaria
    diagnosis. <i>PloS one</i>, 6(10), e26127. [ <a
    href="http://authors.library.caltech.edu/28338/1/Lee2011p16362PLoS_ONE.pdf">PDF</a>
    | <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026127">link</a>
    ]</li>
        

    <li>Zheng, G.*, Lee, S. A.*, <u>Yang, S.*</u>, & Yang, C. (2010). Sub-pixel
    resolving optofluidic microscope for on-chip cell imaging. <i>Lab on
    a Chip</i>, 10(22), 3125-3129. *co-first authors [ <a
    href="http://authors.library.caltech.edu/20266/1/Zheng2010p11478Lab_on_a_Chip.pdf">PDF</a>
    | <a href="http://pubs.rsc.org/en/Content/ArticleLanding/2010/LC/c0lc00213e">link</a>
    ]</li>
      


</ol>
</p><strong>Ph.D. Thesis</strong>: Coded Computational Illumination
and Detection for Three-dimensional Fluorescence Microscopy [ <a
href="https://stacks.stanford.edu/file/druid:ry156md3365/Sam_thesis_draft_2016_05-27-augmented.pdf">PDF</a> | <a
href="https://searchworks.stanford.edu/view/11686015">summary</a> ]
</p><strong>Unpublished graduate work</strong> includes <a
href="http://image-sensors-world.blogspot.com/2015/04/pelican-imaging-proposes-depth-assisted.html">depth-assisted
perspective correction</a> for portrait photography, <a
href="http://www.abstractsonline.com/Plan/ViewAbstract.aspx?sKey=6a63415c-2cc3-4ab1-8754-e660b643739f&cKey=acac2184-02f4-48f4-913b-2033b8186613&mKey=54c85d94-6d69-4b09-afaa-502c0e680ca7">holographic
illumination</a> for all-optical neurophysiology, the <a
href="http://clarityresourcecenter.com/SWIFT3D/Grosenick_Broxton_Kim_Liston_etal_poster_SfN2013.jpg">application of light
field microscopy to 3D calcium imaging</a>, and a
<a
href="http://samueljyang.xyz/pdfs/Yang_2012_unpublished.pdf">robust
real time zebrafish high speed tail tracking approach</a> (computer
vision and machine learning in OpenCV/Matlab) for zebrafish virtual reality.</p>

                </div>
            </div>
        </div>
    </section>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Scrolling Nav JavaScript -->
    <script src="js/jquery.easing.min.js"></script>
    <script src="js/scrolling-nav.js"></script>

</body>

</html>
